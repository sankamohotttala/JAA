{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Node Classification using Graph Convolutional Networks </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This node classification task uses CORA dataset from https://linqs.soe.ucsc.edu/data<br>\n",
    "The dataset consists of <b>2708</b> nodes which correspond to scientific publications.<br>\n",
    "The nodes are classified into <b>7</b> categories indicating the topics of each document.<br>\n",
    "The edges indicate whether a document is cited by the other or vice versa.<br>\n",
    "Each node has <b>1433</b> features which is described by a 0/1-valued vector, indicating the bag-of-words from the dictionary.<br>\n",
    "\n",
    "<br> This is an undirected graph problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from spektral.layers import GraphConv\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Loading and Preprocessing </h2><br>\n",
    "We are going to use the edges connecting the  (from file <b>cora.cites</b>).<br>\n",
    "The nodes are loaded from file <b>cora.content</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cora   []   ['cora.cites', 'cora.content', 'cora.edges', 'cora.node_labels', 'README', 'readme.html']\n"
     ]
    }
   ],
   "source": [
    "#loading the data\n",
    "\n",
    "all_data = []\n",
    "all_edges = []\n",
    "\n",
    "for root,dirs,files in os.walk('./cora'):\n",
    "    print(root,' ',dirs,' ',files)\n",
    "    for file in files:\n",
    "        #print(file)\n",
    "        if '.content' in file:\n",
    "            with open(os.path.join(root,file),'r') as f:\n",
    "                all_data.extend(f.read().splitlines()) #splits the files by new line(\\n)\n",
    "        elif 'cites' in file:\n",
    "            with open(os.path.join(root,file),'r') as f:\n",
    "                all_edges.extend(f.read().splitlines())\n",
    "\n",
    "                \n",
    "#Shuffle the data because the raw data is ordered based on the label\n",
    "random_state = 77\n",
    "all_data = shuffle(all_data,random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2708\n",
      "5429\n",
      "504\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\tProbabilistic_Methods\n"
     ]
    }
   ],
   "source": [
    "print(len(all_data))\n",
    "print(len(all_edges))\n",
    "type(all_edges[0])\n",
    "print((all_data[110]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <b>cora.content</b> file:<br>\n",
    "The <b>first</b> element indicates the <b>node name</b><br>\n",
    "The <b>second</b> until the last second elements indicate the <b>node features</b><br>\n",
    "The <b>last</b> element indicates the <b>label of that particular node</b><br>\n",
    "\n",
    "In <b>cora.cites</b> file:<br>\n",
    "Each line indicates the tuple of connected nodes\n",
    "\n",
    "\n",
    "<h3>Parsing the data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (2708, 1433)\n",
      "\n",
      "Number of nodes (N):  2708\n",
      "Number of features (F) of each node:  1433\n",
      "\n",
      "Categories:  {'Theory', 'Neural_Networks', 'Reinforcement_Learning', 'Rule_Learning', 'Genetic_Algorithms', 'Case_Based', 'Probabilistic_Methods'}\n",
      "\n",
      "Number of classes:  7\n"
     ]
    }
   ],
   "source": [
    "#parse the data\n",
    "labels = []\n",
    "nodes = []\n",
    "X = []\n",
    "\n",
    "for i,data in enumerate(all_data):\n",
    "    elements = data.split('\\t')\n",
    "    labels.append(elements[-1])\n",
    "    X.append(elements[1:-1])\n",
    "    nodes.append(elements[0])\n",
    "#print(elements)\n",
    "X = np.array(X,dtype=int) #node feature matrix??\n",
    "N = X.shape[0] #the number of nodes\n",
    "F = X.shape[1] #the size of node features\n",
    "print('X shape: ', X.shape)\n",
    "\n",
    "\n",
    "#parse the edge\n",
    "edge_list=[]\n",
    "for edge in all_edges:\n",
    "    e = edge.split('\\t')\n",
    "    edge_list.append((e[0],e[1]))# string to tuple conversion\n",
    "\n",
    "print('\\nNumber of nodes (N): ', N)\n",
    "print('Number of features (F) of each node: ', F)\n",
    "print('\\nCategories: ', set(labels)) #set gives the unique labels\n",
    "\n",
    "num_classes = len(set(labels))\n",
    "print('\\nNumber of classes: ', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Select examples for training, validation, and test then set the mask</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_counter:  {'Reinforcement_Learning': 0, 'Probabilistic_Methods': 0, 'Neural_Networks': 0, 'Case_Based': 0, 'Theory': 0, 'Genetic_Algorithms': 0, 'Rule_Learning': 0}\n",
      "2708\n",
      "[73, 80, 92, 98, 99, 103, 106, 107, 111, 112]\n"
     ]
    }
   ],
   "source": [
    "def limit_data(labels,limit=20,val_num=500,test_num=1000):\n",
    "    '''\n",
    "    Get the index of train, validation, and test data\n",
    "    '''\n",
    "    label_counter = dict((l, 0) for l in labels)\n",
    "    print('label_counter: ',label_counter)\n",
    "    train_idx = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i]\n",
    "        if label_counter[label]<limit:\n",
    "            #add the example to the training data\n",
    "            train_idx.append(i)\n",
    "            label_counter[label]+=1\n",
    "        \n",
    "        #exit the loop once we found 20 examples for each class\n",
    "        if all(count == limit for count in label_counter.values()):\n",
    "            break\n",
    "    \n",
    "    #get the indices that do not go to traning data\n",
    "    rest_idx = [x for x in range(len(labels)) if x not in train_idx]\n",
    "    print(len(labels))\n",
    "    print((rest_idx[:10]))\n",
    "    #get the first val_num\n",
    "    val_idx = rest_idx[:val_num] # from 0 to val_num-1\n",
    "    test_idx = rest_idx[val_num:(val_num+test_num)]\n",
    "    return train_idx, val_idx,test_idx\n",
    "\n",
    "\n",
    "train_idx,val_idx,test_idx = limit_data(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the mask\n",
    "train_mask = np.zeros((N,),dtype=bool)\n",
    "train_mask[train_idx] = True # with this way we can modify multiple numpy array elements at once  \n",
    "\n",
    "val_mask = np.zeros((N,),dtype=bool)\n",
    "val_mask[val_idx] = True\n",
    "\n",
    "test_mask = np.zeros((N,),dtype=bool)\n",
    "test_mask[test_idx] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ..., False, False, False])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Show Data Distribution</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Data Distribution: \n",
      "Counter({'Neural_Networks': 818, 'Probabilistic_Methods': 426, 'Genetic_Algorithms': 418, 'Theory': 351, 'Case_Based': 298, 'Reinforcement_Learning': 217, 'Rule_Learning': 180})\n"
     ]
    }
   ],
   "source": [
    "print(\"All Data Distribution: \\n{}\".format(Counter(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Distribution: \n",
      "Counter({'Reinforcement_Learning': 20, 'Probabilistic_Methods': 20, 'Neural_Networks': 20, 'Case_Based': 20, 'Theory': 20, 'Genetic_Algorithms': 20, 'Rule_Learning': 20})\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Data Distribution: \\n{}\".format(Counter([labels[i] for i in train_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Data Distribution: \n",
      "Counter({'Neural_Networks': 172, 'Genetic_Algorithms': 78, 'Probabilistic_Methods': 72, 'Theory': 63, 'Case_Based': 58, 'Reinforcement_Learning': 35, 'Rule_Learning': 22})\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Data Distribution: \\n{}\".format(Counter([labels[i] for i in val_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing Data Distribution: \n",
      "Counter({'Neural_Networks': 290, 'Probabilistic_Methods': 172, 'Genetic_Algorithms': 156, 'Theory': 123, 'Case_Based': 114, 'Reinforcement_Learning': 85, 'Rule_Learning': 60})\n"
     ]
    }
   ],
   "source": [
    "print(\"testing Data Distribution: \\n{}\".format(Counter([labels[i] for i in test_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Convert the labels to one hot encoding</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelEncoder()\n",
      "[4 3 3 ... 2 6 2]\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def encode_label(labels):\n",
    "    label_encoder = LabelEncoder()#LabelEncoder is a utility class to help normalize labels such that they contain only values between 0 and n_classes-1. \n",
    "    print(label_encoder)\n",
    "    labels = label_encoder.fit_transform(labels)#check the difference between of fit_tf() and tf()\n",
    "    print(labels)\n",
    "    labels = to_categorical(labels)\n",
    "    print(labels)\n",
    "    return labels, label_encoder.classes_\n",
    "#print(labels)\n",
    "labels_encoded, classes = encode_label(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Case_Based', 'Genetic_Algorithms', 'Neural_Networks',\n",
       "       'Probabilistic_Methods', 'Reinforcement_Learning', 'Rule_Learning',\n",
       "       'Theory'], dtype='<U22')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Build a graph on NetworkX using the obtained nodes and edges list</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph info:  Name: \n",
      "Type: Graph\n",
      "Number of nodes: 2708\n",
      "Number of edges: 5278\n",
      "Average degree:   3.8981\n"
     ]
    }
   ],
   "source": [
    "#build the graph\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edge_list) #list of tuples\n",
    "\n",
    "#obtain the adjacency matrix (A)\n",
    "A = nx.adjacency_matrix(G)\n",
    "print('Graph info: ', nx.info(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Building and Training Graph Convolutional Networks </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1433)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1433)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 2708)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv (GraphConv)          (None, 16)           22928       dropout[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 16)           0           graph_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "graph_conv_1 (GraphConv)        (None, 7)            112         dropout_1[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,040\n",
      "Trainable params: 23,040\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "channels = 16           # Number of channels in the first layer\n",
    "dropout = 0.5           # Dropout rate for the features\n",
    "l2_reg = 5e-4           # L2 regularization rate\n",
    "learning_rate = 1e-2    # Learning rate\n",
    "epochs = 200            # Number of training epochs\n",
    "es_patience = 10        # Patience for early stopping\n",
    "\n",
    "# Preprocessing operations\n",
    "A = GraphConv.preprocess(A).astype('f4')\n",
    "\n",
    "# Model definition\n",
    "X_in = Input(shape=(F, )) #F is number of features 1430+??\n",
    "fltr_in = Input((N, ), sparse=True)\n",
    "\n",
    "dropout_1 = Dropout(dropout)(X_in)\n",
    "graph_conv_1 = GraphConv(channels,\n",
    "                         activation='relu',\n",
    "                         kernel_regularizer=l2(l2_reg),\n",
    "                         use_bias=False)([dropout_1, fltr_in])\n",
    "\n",
    "dropout_2 = Dropout(dropout)(graph_conv_1)\n",
    "graph_conv_2 = GraphConv(num_classes,\n",
    "                         activation='softmax',\n",
    "                         use_bias=False)([dropout_2, fltr_in]) #num_classes =7\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, fltr_in], outputs=graph_conv_2)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              weighted_metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "tbCallBack_GCN = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./Tensorboard_GCN_cora',\n",
    ")\n",
    "callback_GCN = [tbCallBack_GCN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.callbacks.TensorBoard at 0x199990d1f98>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback_GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 0.1167 - acc: 0.1500 - val_loss: 0.3657 - val_acc: 0.2540\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 498ms/step - loss: 0.1095 - acc: 0.3071 - val_loss: 0.3563 - val_acc: 0.3120\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.1032 - acc: 0.4071 - val_loss: 0.3468 - val_acc: 0.3600\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0976 - acc: 0.5000 - val_loss: 0.3378 - val_acc: 0.3860\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.0932 - acc: 0.4929 - val_loss: 0.3285 - val_acc: 0.4140\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 0.0890 - acc: 0.6071 - val_loss: 0.3200 - val_acc: 0.4440\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.0855 - acc: 0.6929 - val_loss: 0.3118 - val_acc: 0.4960\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0808 - acc: 0.7143 - val_loss: 0.3039 - val_acc: 0.5460\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 0.0781 - acc: 0.7500 - val_loss: 0.2964 - val_acc: 0.5780\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 0.0751 - acc: 0.8071 - val_loss: 0.2887 - val_acc: 0.6140\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 0.0731 - acc: 0.8786 - val_loss: 0.2812 - val_acc: 0.6460\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 0.0729 - acc: 0.8571 - val_loss: 0.2739 - val_acc: 0.6780\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.0687 - acc: 0.8714 - val_loss: 0.2667 - val_acc: 0.6900\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.0686 - acc: 0.8786 - val_loss: 0.2596 - val_acc: 0.7000\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 0.0671 - acc: 0.8786 - val_loss: 0.2529 - val_acc: 0.7160\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 0.0632 - acc: 0.9143 - val_loss: 0.2464 - val_acc: 0.7260\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0629 - acc: 0.8929 - val_loss: 0.2405 - val_acc: 0.7260\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 0.0604 - acc: 0.9286 - val_loss: 0.2351 - val_acc: 0.7400\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0604 - acc: 0.9071 - val_loss: 0.2300 - val_acc: 0.7440\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.0601 - acc: 0.9214 - val_loss: 0.2255 - val_acc: 0.7560\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.0587 - acc: 0.9071 - val_loss: 0.2215 - val_acc: 0.7600\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.0577 - acc: 0.9071 - val_loss: 0.2180 - val_acc: 0.7660\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0558 - acc: 0.9071 - val_loss: 0.2147 - val_acc: 0.7700\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0544 - acc: 0.9429 - val_loss: 0.2116 - val_acc: 0.7720\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0514 - acc: 0.9500 - val_loss: 0.2086 - val_acc: 0.7700\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0520 - acc: 0.9357 - val_loss: 0.2056 - val_acc: 0.7680\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0520 - acc: 0.9214 - val_loss: 0.2026 - val_acc: 0.7700\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0518 - acc: 0.9214 - val_loss: 0.1991 - val_acc: 0.7680\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 0.0526 - acc: 0.9286 - val_loss: 0.1960 - val_acc: 0.7720\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0495 - acc: 0.9357 - val_loss: 0.1929 - val_acc: 0.7740\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.0474 - acc: 0.9500 - val_loss: 0.1903 - val_acc: 0.7720\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0475 - acc: 0.9286 - val_loss: 0.1878 - val_acc: 0.7760\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0485 - acc: 0.9357 - val_loss: 0.1860 - val_acc: 0.7780\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0467 - acc: 0.9500 - val_loss: 0.1850 - val_acc: 0.7780\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 0.0441 - acc: 0.9500 - val_loss: 0.1848 - val_acc: 0.7780\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 0.0451 - acc: 0.9429 - val_loss: 0.1848 - val_acc: 0.7680\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0459 - acc: 0.9143 - val_loss: 0.1848 - val_acc: 0.7660\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.0431 - acc: 0.9571 - val_loss: 0.1842 - val_acc: 0.7720\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0440 - acc: 0.9429 - val_loss: 0.1833 - val_acc: 0.7700\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0419 - acc: 0.9571 - val_loss: 0.1812 - val_acc: 0.7740\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0420 - acc: 0.9571 - val_loss: 0.1787 - val_acc: 0.7780\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0451 - acc: 0.9286 - val_loss: 0.1770 - val_acc: 0.7840\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0431 - acc: 0.9143 - val_loss: 0.1746 - val_acc: 0.7900\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0412 - acc: 0.9500 - val_loss: 0.1721 - val_acc: 0.7880\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.0431 - acc: 0.9571 - val_loss: 0.1701 - val_acc: 0.7940\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0409 - acc: 0.9500 - val_loss: 0.1687 - val_acc: 0.7920\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0400 - acc: 0.9500 - val_loss: 0.1680 - val_acc: 0.7900\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0379 - acc: 0.9643 - val_loss: 0.1674 - val_acc: 0.7920\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0412 - acc: 0.9429 - val_loss: 0.1673 - val_acc: 0.7900\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0400 - acc: 0.9500 - val_loss: 0.1675 - val_acc: 0.7820\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0389 - acc: 0.9429 - val_loss: 0.1677 - val_acc: 0.7820\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0400 - acc: 0.9357 - val_loss: 0.1675 - val_acc: 0.7860\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0362 - acc: 0.9643 - val_loss: 0.1671 - val_acc: 0.7900\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.0393 - acc: 0.9429 - val_loss: 0.1668 - val_acc: 0.7960\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.0361 - acc: 0.9643 - val_loss: 0.1662 - val_acc: 0.7940\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0355 - acc: 0.9714 - val_loss: 0.1659 - val_acc: 0.7900\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0385 - acc: 0.9429 - val_loss: 0.1653 - val_acc: 0.7860\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0357 - acc: 0.9786 - val_loss: 0.1645 - val_acc: 0.7880\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0360 - acc: 0.9714 - val_loss: 0.1635 - val_acc: 0.7840\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 168ms/step - loss: 0.0366 - acc: 0.9500 - val_loss: 0.1624 - val_acc: 0.7820\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0368 - acc: 0.9571 - val_loss: 0.1615 - val_acc: 0.7920\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0375 - acc: 0.9571 - val_loss: 0.1610 - val_acc: 0.7880\n",
      "Epoch 63/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 193ms/step - loss: 0.0358 - acc: 0.9429 - val_loss: 0.1600 - val_acc: 0.7860\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0334 - acc: 0.9786 - val_loss: 0.1598 - val_acc: 0.7820\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0336 - acc: 0.9571 - val_loss: 0.1592 - val_acc: 0.7840\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0369 - acc: 0.9571 - val_loss: 0.1593 - val_acc: 0.7800\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0334 - acc: 0.9643 - val_loss: 0.1591 - val_acc: 0.7820\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0358 - acc: 0.9500 - val_loss: 0.1586 - val_acc: 0.7860\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0332 - acc: 0.9857 - val_loss: 0.1581 - val_acc: 0.7900\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 0.0336 - acc: 0.9714 - val_loss: 0.1570 - val_acc: 0.7880\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0365 - acc: 0.9500 - val_loss: 0.1558 - val_acc: 0.7860\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0342 - acc: 0.9571 - val_loss: 0.1551 - val_acc: 0.7880\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0354 - acc: 0.9714 - val_loss: 0.1548 - val_acc: 0.7880\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0343 - acc: 0.9571 - val_loss: 0.1557 - val_acc: 0.7900\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0374 - acc: 0.9214 - val_loss: 0.1564 - val_acc: 0.7900\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0320 - acc: 0.9786 - val_loss: 0.1567 - val_acc: 0.7880\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.0328 - acc: 0.9786 - val_loss: 0.1565 - val_acc: 0.7840\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0313 - acc: 1.0000 - val_loss: 0.1561 - val_acc: 0.7840\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0350 - acc: 0.9429 - val_loss: 0.1555 - val_acc: 0.7820\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 172ms/step - loss: 0.0321 - acc: 0.9786 - val_loss: 0.1552 - val_acc: 0.7840\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0328 - acc: 0.9500 - val_loss: 0.1547 - val_acc: 0.7760\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0330 - acc: 0.9571 - val_loss: 0.1550 - val_acc: 0.7760\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0317 - acc: 0.9786 - val_loss: 0.1552 - val_acc: 0.7760\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0327 - acc: 0.9571 - val_loss: 0.1554 - val_acc: 0.7680\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0297 - acc: 0.9929 - val_loss: 0.1555 - val_acc: 0.7620\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0327 - acc: 0.9429 - val_loss: 0.1545 - val_acc: 0.7680\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0330 - acc: 0.9643 - val_loss: 0.1529 - val_acc: 0.7760\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0312 - acc: 0.9571 - val_loss: 0.1520 - val_acc: 0.7780\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0305 - acc: 0.9571 - val_loss: 0.1517 - val_acc: 0.7820\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 175ms/step - loss: 0.0311 - acc: 0.9786 - val_loss: 0.1526 - val_acc: 0.7760\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0302 - acc: 0.9929 - val_loss: 0.1528 - val_acc: 0.7800\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0296 - acc: 0.9786 - val_loss: 0.1529 - val_acc: 0.7760\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0317 - acc: 0.9429 - val_loss: 0.1532 - val_acc: 0.7760\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 182ms/step - loss: 0.0301 - acc: 0.9786 - val_loss: 0.1543 - val_acc: 0.7680\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 167ms/step - loss: 0.0308 - acc: 0.9500 - val_loss: 0.1543 - val_acc: 0.7660\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0303 - acc: 0.9857 - val_loss: 0.1540 - val_acc: 0.7620\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 0.0285 - acc: 0.9643 - val_loss: 0.1529 - val_acc: 0.7700\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0306 - acc: 0.9357 - val_loss: 0.1522 - val_acc: 0.7680\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0320 - acc: 0.9571 - val_loss: 0.1511 - val_acc: 0.7680\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 173ms/step - loss: 0.0305 - acc: 0.9429 - val_loss: 0.1500 - val_acc: 0.7780\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 177ms/step - loss: 0.0284 - acc: 0.9786 - val_loss: 0.1491 - val_acc: 0.7780\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 174ms/step - loss: 0.0289 - acc: 0.9714 - val_loss: 0.1484 - val_acc: 0.7800\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 0.0299 - acc: 0.9429 - val_loss: 0.1482 - val_acc: 0.7860\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 179ms/step - loss: 0.0301 - acc: 0.9571 - val_loss: 0.1496 - val_acc: 0.7800\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0290 - acc: 0.9571 - val_loss: 0.1516 - val_acc: 0.7740\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0307 - acc: 0.9643 - val_loss: 0.1532 - val_acc: 0.7580\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 171ms/step - loss: 0.0275 - acc: 0.9857 - val_loss: 0.1539 - val_acc: 0.7540\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0281 - acc: 0.9857 - val_loss: 0.1537 - val_acc: 0.7480\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0314 - acc: 0.9500 - val_loss: 0.1523 - val_acc: 0.7480\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.0285 - acc: 0.9571 - val_loss: 0.1498 - val_acc: 0.7560\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 0.0304 - acc: 0.9500 - val_loss: 0.1483 - val_acc: 0.7600\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 0.0281 - acc: 0.9786 - val_loss: 0.1471 - val_acc: 0.7760\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0288 - acc: 0.9643 - val_loss: 0.1470 - val_acc: 0.7820\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0293 - acc: 0.9786 - val_loss: 0.1480 - val_acc: 0.7780\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 176ms/step - loss: 0.0287 - acc: 0.9571 - val_loss: 0.1491 - val_acc: 0.7700\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 0.0302 - acc: 0.9286 - val_loss: 0.1506 - val_acc: 0.7640\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 0.0249 - acc: 0.9786 - val_loss: 0.1517 - val_acc: 0.7560\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 0.0302 - acc: 0.9643 - val_loss: 0.1520 - val_acc: 0.7560\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0302 - acc: 0.9357 - val_loss: 0.1533 - val_acc: 0.7560\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0284 - acc: 0.9786 - val_loss: 0.1530 - val_acc: 0.7600\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 0.0284 - acc: 0.9714 - val_loss: 0.1508 - val_acc: 0.7640\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 0.0253 - acc: 1.0000 - val_loss: 0.1482 - val_acc: 0.7660\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 0.0279 - acc: 0.9857 - val_loss: 0.1446 - val_acc: 0.7800\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.0280 - acc: 0.9857 - val_loss: 0.1428 - val_acc: 0.7820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0289 - acc: 0.9786 - val_loss: 0.1429 - val_acc: 0.7840\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.0267 - acc: 0.9786 - val_loss: 0.1436 - val_acc: 0.7740\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.0270 - acc: 0.9643 - val_loss: 0.1450 - val_acc: 0.7740\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 0.0266 - acc: 0.9571 - val_loss: 0.1475 - val_acc: 0.7740\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0273 - acc: 0.9857 - val_loss: 0.1491 - val_acc: 0.7660\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0277 - acc: 0.9571 - val_loss: 0.1494 - val_acc: 0.7640\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0265 - acc: 0.9643 - val_loss: 0.1475 - val_acc: 0.7680\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 0.0262 - acc: 0.9786 - val_loss: 0.1464 - val_acc: 0.7680\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 178ms/step - loss: 0.0282 - acc: 0.9357 - val_loss: 0.1451 - val_acc: 0.7700\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 185ms/step - loss: 0.0261 - acc: 0.9786 - val_loss: 0.1445 - val_acc: 0.7720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19995cccdd8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "validation_data = ([X, A], labels_encoded, val_mask)\n",
    "model.fit([X, A],\n",
    "          labels_encoded,\n",
    "          sample_weight=train_mask,\n",
    "          epochs=epochs,\n",
    "          batch_size=N,\n",
    "          validation_data=validation_data,\n",
    "          shuffle=False,\n",
    "          callbacks=[\n",
    "              EarlyStopping(patience=es_patience,  restore_best_weights=True),\n",
    "              tbCallBack_GCN\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2708"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "X_te = X[test_mask]\n",
    "A_te = A[test_mask,:][:,test_mask]\n",
    "y_te = labels_encoded[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(A_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n        return self(x, training=False)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:274 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 1 is incompatible with layer model: expected shape=(None, 2708), found shape=(None, 1000)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3f83555e3485>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_te\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA_te\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GCN Classification Report: \\n {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1629\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    869\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 726\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2968\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2969\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2970\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2971\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3361\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3206\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1434 predict_step\n        return self(x, training=False)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\Sanka Mohottala\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:274 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 1 is incompatible with layer model: expected shape=(None, 2708), found shape=(None, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "X_te = X[test_mask]\n",
    "A_te = A[test_mask,:][:,test_mask]\n",
    "y_te = labels_encoded[test_mask]\n",
    "\n",
    "y_pred = model.predict([X_te, A_te], batch_size=N)\n",
    "report = classification_report(np.argmax(y_te,axis=1), np.argmax(y_pred,axis=1), target_names=classes)\n",
    "print('GCN Classification Report: \\n {}'.format(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Get hidden layer representation for GCN </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict([X,A],batch_size=N)\n",
    "\n",
    "#Get t-SNE Representation\n",
    "#get the hidden layer representation after the first GCN layer\n",
    "x_tsne = TSNE(n_components=2).fit_transform(activations[3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tSNE(labels_encoded,x_tsne):\n",
    "    color_map = np.argmax(labels_encoded, axis=1)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for cl in range(num_classes):\n",
    "        indices = np.where(color_map==cl)\n",
    "        indices = indices[0]\n",
    "        plt.scatter(x_tsne[indices,0], x_tsne[indices, 1], label=cl)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_tSNE(labels_encoded,x_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Comparison to Fully-Connected Neural Networks </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Building and Training FNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_patience = 10\n",
    "optimizer = Adam(lr=1e-2)\n",
    "l2_reg = 5e-4\n",
    "epochs = 200\n",
    "\n",
    "#Compare with FNN\n",
    "#Construct the model\n",
    "model_fnn = Sequential()\n",
    "model_fnn.add(Dense(\n",
    "                    128,\n",
    "                    input_dim=X.shape[1],\n",
    "                    activation=tf.nn.relu,\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(l2_reg))\n",
    "             )\n",
    "model_fnn.add(Dropout(0.5))\n",
    "model_fnn.add(Dense(256, activation=tf.nn.relu))\n",
    "model_fnn.add(Dropout(0.5))\n",
    "model_fnn.add(Dense(num_classes, activation=tf.keras.activations.softmax))\n",
    "\n",
    "\n",
    "model_fnn.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              weighted_metrics=['acc'])\n",
    "\n",
    "\n",
    "#define TensorBoard\n",
    "tbCallBack_FNN = TensorBoard(\n",
    "    log_dir='./Tensorboard_FNN_cora',\n",
    ")\n",
    "\n",
    "#Train model\n",
    "validation_data_fnn = (X, labels_encoded, val_mask)\n",
    "model_fnn.fit(\n",
    "                X,labels_encoded,\n",
    "                sample_weight=train_mask,\n",
    "                epochs=epochs,\n",
    "                batch_size=N,\n",
    "                validation_data=validation_data_fnn,\n",
    "                shuffle=False,\n",
    "                callbacks=[\n",
    "                  EarlyStopping(patience=es_patience,  restore_best_weights=True),\n",
    "                  tbCallBack_FNN\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = model_fnn.predict(X_te)\n",
    "report = classification_report(np.argmax(y_te,axis=1), np.argmax(y_pred,axis=1), target_names=classes)\n",
    "print('FCNN Classification Report: \\n {}'.format(report))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Get hidden layer representation for FNN</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in model_fnn.layers] \n",
    "activation_model = Model(inputs=model_fnn.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict([X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tsne = TSNE(n_components=2).fit_transform(activations[3])\n",
    "plot_tSNE(labels_encoded,x_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### END OF NOTEBOOK ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
